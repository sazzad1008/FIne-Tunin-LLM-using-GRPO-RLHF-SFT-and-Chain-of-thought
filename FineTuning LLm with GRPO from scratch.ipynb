{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Import PyTorch and Hugging Face Transformers\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# Import dataset utilities\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import libraries from TRL (Transformers Reinforcement Learning)\n",
    "from trl import (\n",
    "    AutoModelForCausalLMWithValueHead, \n",
    "    PPOConfig, \n",
    "    PPOTrainer, \n",
    "    GRPOTrainer, \n",
    "    GRPOConfig, \n",
    "    SFTTrainer\n",
    ")\n",
    "\n",
    "\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 151665\n",
      "Model max length: 131072\n",
      "Pad token: <|endoftext|>\n",
      "EOS token: <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "OUTPUT_DIR = \"/tmp/Qwen-GRPO-training\" # For saving our trained model\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize tokenizer with chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9331c4bbe64ae08de20d6dbc0f4db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 494,032,768\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Test Input: how are you?\n",
      "Model Response: system\n",
      "You are Qwen, a helpful assistant.\n",
      "user\n",
      "how are you?\n",
      "assistant\n",
      "Hello! I'm just an AI language model created by Alibaba Cloud. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Test basic inference\n",
    "def test_model_inference(user_input: str):\n",
    "    \"\"\"Test basic model inference with the loaded model and tokenizer.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "test_input = \"how are you?\"\n",
    "response = test_model_inference(test_input)\n",
    "print(f\"Test Input: {test_input}\")\n",
    "print(f\"Model Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")\n",
    "def make_conversation(example):\n",
    "    \"\"\"Convert dataset examples into conversation format.\"\"\"\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'What is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.',\n",
       " 'solution': \"To determine the coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\), we can use the binomial theorem.\\n\\nThe binomial theorem states:\\n\\\\[\\n(a + b)^n = \\\\sum_{k=0}^{n} \\\\binom{n}{k} a^{n-k} b^k\\n\\\\]\\n\\nIn this case, \\\\(a = \\\\frac{3}{5}x\\\\), \\\\(b = -\\\\frac{y}{2}\\\\), and \\\\(n = 8\\\\).\\n\\nWe are interested in the term that contains \\\\(x^2y^6\\\\). In the general term of the binomial expansion:\\n\\\\[\\n\\\\binom{8}{k} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-k} \\\\left(-\\\\frac{y}{2}\\\\right)^k\\n\\\\]\\n\\nTo get \\\\(x^2\\\\), we need \\\\(8 - k = 2\\\\), thus \\\\(k = 6\\\\).\\n\\nSubstituting \\\\(k = 6\\\\) into the expression:\\n\\\\[\\n\\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-6} \\\\left(-\\\\frac{y}{2}\\\\right)^6 = \\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^2 \\\\left(-\\\\frac{y}{2}\\\\right)^6\\n\\\\]\\n\\nNow, we will compute each part of this expression.\\n\\n1. Calculate the binomial coefficient \\\\(\\\\binom{8}{6}\\\\).\\n2. Compute \\\\(\\\\left(\\\\frac{3}{5}\\\\right)^2\\\\).\\n3. Compute \\\\(\\\\left(-\\\\frac{y}{2}\\\\right)^6\\\\).\\n4. Combine everything together to get the coefficient of \\\\(x^2y^6\\\\).\\n\\nLet's compute these in Python.\\n```python\\nfrom math import comb\\n\\n# Given values\\nn = 8\\nk = 6\\n\\n# Calculate the binomial coefficient\\nbinom_coeff = comb(n, k)\\n\\n# Compute (3/5)^2\\na_term = (3/5)**2\\n\\n# Compute (-1/2)^6\\nb_term = (-1/2)**6\\n\\n# Combine terms to get the coefficient of x^2y^6\\ncoefficient = binom_coeff * a_term * b_term\\nprint(coefficient)\\n```\\n```output\\n0.1575\\n```\\nThe coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\) is \\\\(0.1575\\\\). To express this as a common fraction, we recognize that:\\n\\n\\\\[ 0.1575 = \\\\frac{1575}{10000} = \\\\frac{63}{400} \\\\]\\n\\nThus, the coefficient can be expressed as:\\n\\n\\\\[\\n\\\\boxed{\\\\frac{63}{400}}\\n\\\\]\",\n",
       " 'messages': [{'content': 'What is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.',\n",
       "   'role': 'user'},\n",
       "  {'content': \"To determine the coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\), we can use the binomial theorem.\\n\\nThe binomial theorem states:\\n\\\\[\\n(a + b)^n = \\\\sum_{k=0}^{n} \\\\binom{n}{k} a^{n-k} b^k\\n\\\\]\\n\\nIn this case, \\\\(a = \\\\frac{3}{5}x\\\\), \\\\(b = -\\\\frac{y}{2}\\\\), and \\\\(n = 8\\\\).\\n\\nWe are interested in the term that contains \\\\(x^2y^6\\\\). In the general term of the binomial expansion:\\n\\\\[\\n\\\\binom{8}{k} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-k} \\\\left(-\\\\frac{y}{2}\\\\right)^k\\n\\\\]\\n\\nTo get \\\\(x^2\\\\), we need \\\\(8 - k = 2\\\\), thus \\\\(k = 6\\\\).\\n\\nSubstituting \\\\(k = 6\\\\) into the expression:\\n\\\\[\\n\\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-6} \\\\left(-\\\\frac{y}{2}\\\\right)^6 = \\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^2 \\\\left(-\\\\frac{y}{2}\\\\right)^6\\n\\\\]\\n\\nNow, we will compute each part of this expression.\\n\\n1. Calculate the binomial coefficient \\\\(\\\\binom{8}{6}\\\\).\\n2. Compute \\\\(\\\\left(\\\\frac{3}{5}\\\\right)^2\\\\).\\n3. Compute \\\\(\\\\left(-\\\\frac{y}{2}\\\\right)^6\\\\).\\n4. Combine everything together to get the coefficient of \\\\(x^2y^6\\\\).\\n\\nLet's compute these in Python.\\n```python\\nfrom math import comb\\n\\n# Given values\\nn = 8\\nk = 6\\n\\n# Calculate the binomial coefficient\\nbinom_coeff = comb(n, k)\\n\\n# Compute (3/5)^2\\na_term = (3/5)**2\\n\\n# Compute (-1/2)^6\\nb_term = (-1/2)**6\\n\\n# Combine terms to get the coefficient of x^2y^6\\ncoefficient = binom_coeff * a_term * b_term\\nprint(coefficient)\\n```\\n```output\\n0.1575\\n```\\nThe coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\) is \\\\(0.1575\\\\). To express this as a common fraction, we recognize that:\\n\\n\\\\[ 0.1575 = \\\\frac{1575}{10000} = \\\\frac{63}{400} \\\\]\\n\\nThus, the coefficient can be expressed as:\\n\\n\\\\[\\n\\\\boxed{\\\\frac{63}{400}}\\n\\\\]\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the \"AI-MO/NuminaMath-TIR\" dataset from DigitalLearningGmbH\n",
    "MATH_le = load_dataset(\"AI-MO/NuminaMath-TIR\", \"default\")  \n",
    "\n",
    "# Access the first sample in the training set\n",
    "MATH_le['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "        \"AI-MO/NuminaMath-TIR\",\n",
    "        name=\"default\",\n",
    "        split=['train', 'test']\n",
    "    )\n",
    "    \n",
    "    # Convert splits into dictionary\n",
    "dataset = {\n",
    "    'train': dataset[0],\n",
    "    'test': dataset[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['problem', 'solution', 'messages']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].map(make_conversation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'What is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.',\n",
       " 'solution': \"To determine the coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\), we can use the binomial theorem.\\n\\nThe binomial theorem states:\\n\\\\[\\n(a + b)^n = \\\\sum_{k=0}^{n} \\\\binom{n}{k} a^{n-k} b^k\\n\\\\]\\n\\nIn this case, \\\\(a = \\\\frac{3}{5}x\\\\), \\\\(b = -\\\\frac{y}{2}\\\\), and \\\\(n = 8\\\\).\\n\\nWe are interested in the term that contains \\\\(x^2y^6\\\\). In the general term of the binomial expansion:\\n\\\\[\\n\\\\binom{8}{k} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-k} \\\\left(-\\\\frac{y}{2}\\\\right)^k\\n\\\\]\\n\\nTo get \\\\(x^2\\\\), we need \\\\(8 - k = 2\\\\), thus \\\\(k = 6\\\\).\\n\\nSubstituting \\\\(k = 6\\\\) into the expression:\\n\\\\[\\n\\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-6} \\\\left(-\\\\frac{y}{2}\\\\right)^6 = \\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^2 \\\\left(-\\\\frac{y}{2}\\\\right)^6\\n\\\\]\\n\\nNow, we will compute each part of this expression.\\n\\n1. Calculate the binomial coefficient \\\\(\\\\binom{8}{6}\\\\).\\n2. Compute \\\\(\\\\left(\\\\frac{3}{5}\\\\right)^2\\\\).\\n3. Compute \\\\(\\\\left(-\\\\frac{y}{2}\\\\right)^6\\\\).\\n4. Combine everything together to get the coefficient of \\\\(x^2y^6\\\\).\\n\\nLet's compute these in Python.\\n```python\\nfrom math import comb\\n\\n# Given values\\nn = 8\\nk = 6\\n\\n# Calculate the binomial coefficient\\nbinom_coeff = comb(n, k)\\n\\n# Compute (3/5)^2\\na_term = (3/5)**2\\n\\n# Compute (-1/2)^6\\nb_term = (-1/2)**6\\n\\n# Combine terms to get the coefficient of x^2y^6\\ncoefficient = binom_coeff * a_term * b_term\\nprint(coefficient)\\n```\\n```output\\n0.1575\\n```\\nThe coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\) is \\\\(0.1575\\\\). To express this as a common fraction, we recognize that:\\n\\n\\\\[ 0.1575 = \\\\frac{1575}{10000} = \\\\frac{63}{400} \\\\]\\n\\nThus, the coefficient can be expressed as:\\n\\n\\\\[\\n\\\\boxed{\\\\frac{63}{400}}\\n\\\\]\",\n",
       " 'messages': [{'content': 'What is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.',\n",
       "   'role': 'user'},\n",
       "  {'content': \"To determine the coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\), we can use the binomial theorem.\\n\\nThe binomial theorem states:\\n\\\\[\\n(a + b)^n = \\\\sum_{k=0}^{n} \\\\binom{n}{k} a^{n-k} b^k\\n\\\\]\\n\\nIn this case, \\\\(a = \\\\frac{3}{5}x\\\\), \\\\(b = -\\\\frac{y}{2}\\\\), and \\\\(n = 8\\\\).\\n\\nWe are interested in the term that contains \\\\(x^2y^6\\\\). In the general term of the binomial expansion:\\n\\\\[\\n\\\\binom{8}{k} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-k} \\\\left(-\\\\frac{y}{2}\\\\right)^k\\n\\\\]\\n\\nTo get \\\\(x^2\\\\), we need \\\\(8 - k = 2\\\\), thus \\\\(k = 6\\\\).\\n\\nSubstituting \\\\(k = 6\\\\) into the expression:\\n\\\\[\\n\\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-6} \\\\left(-\\\\frac{y}{2}\\\\right)^6 = \\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^2 \\\\left(-\\\\frac{y}{2}\\\\right)^6\\n\\\\]\\n\\nNow, we will compute each part of this expression.\\n\\n1. Calculate the binomial coefficient \\\\(\\\\binom{8}{6}\\\\).\\n2. Compute \\\\(\\\\left(\\\\frac{3}{5}\\\\right)^2\\\\).\\n3. Compute \\\\(\\\\left(-\\\\frac{y}{2}\\\\right)^6\\\\).\\n4. Combine everything together to get the coefficient of \\\\(x^2y^6\\\\).\\n\\nLet's compute these in Python.\\n```python\\nfrom math import comb\\n\\n# Given values\\nn = 8\\nk = 6\\n\\n# Calculate the binomial coefficient\\nbinom_coeff = comb(n, k)\\n\\n# Compute (3/5)^2\\na_term = (3/5)**2\\n\\n# Compute (-1/2)^6\\nb_term = (-1/2)**6\\n\\n# Combine terms to get the coefficient of x^2y^6\\ncoefficient = binom_coeff * a_term * b_term\\nprint(coefficient)\\n```\\n```output\\n0.1575\\n```\\nThe coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\) is \\\\(0.1575\\\\). To express this as a common fraction, we recognize that:\\n\\n\\\\[ 0.1575 = \\\\frac{1575}{10000} = \\\\frac{63}{400} \\\\]\\n\\nThus, the coefficient can be expressed as:\\n\\n\\\\[\\n\\\\boxed{\\\\frac{63}{400}}\\n\\\\]\",\n",
       "   'role': 'assistant'}],\n",
       " 'prompt': [{'content': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>',\n",
       "   'role': 'system'},\n",
       "  {'content': 'What is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.',\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_math_dataset():\n",
    "    \"\"\"Load and prepare the mathematics dataset.\"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"AI-MO/NuminaMath-TIR\",\n",
    "        name=\"default\",\n",
    "        split=['train', 'test']\n",
    "    )\n",
    "    \n",
    "    # Convert splits into dictionary\n",
    "    dataset = {\n",
    "        'train': dataset[0],\n",
    "        'test': dataset[1]\n",
    "    }\n",
    "    \n",
    "    # Apply conversation format\n",
    "    for split in dataset:\n",
    "        dataset[split] = dataset[split].map(make_conversation)\n",
    "\n",
    "        # Remove 'messages' column if exists\n",
    "        if \"messages\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].remove_columns(\"messages\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 72441\n",
      "Test set size: 99\n"
     ]
    }
   ],
   "source": [
    "dataset = load_math_dataset()\n",
    "\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'What is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.',\n",
       " 'solution': \"To determine the coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\), we can use the binomial theorem.\\n\\nThe binomial theorem states:\\n\\\\[\\n(a + b)^n = \\\\sum_{k=0}^{n} \\\\binom{n}{k} a^{n-k} b^k\\n\\\\]\\n\\nIn this case, \\\\(a = \\\\frac{3}{5}x\\\\), \\\\(b = -\\\\frac{y}{2}\\\\), and \\\\(n = 8\\\\).\\n\\nWe are interested in the term that contains \\\\(x^2y^6\\\\). In the general term of the binomial expansion:\\n\\\\[\\n\\\\binom{8}{k} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-k} \\\\left(-\\\\frac{y}{2}\\\\right)^k\\n\\\\]\\n\\nTo get \\\\(x^2\\\\), we need \\\\(8 - k = 2\\\\), thus \\\\(k = 6\\\\).\\n\\nSubstituting \\\\(k = 6\\\\) into the expression:\\n\\\\[\\n\\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^{8-6} \\\\left(-\\\\frac{y}{2}\\\\right)^6 = \\\\binom{8}{6} \\\\left(\\\\frac{3}{5}x\\\\right)^2 \\\\left(-\\\\frac{y}{2}\\\\right)^6\\n\\\\]\\n\\nNow, we will compute each part of this expression.\\n\\n1. Calculate the binomial coefficient \\\\(\\\\binom{8}{6}\\\\).\\n2. Compute \\\\(\\\\left(\\\\frac{3}{5}\\\\right)^2\\\\).\\n3. Compute \\\\(\\\\left(-\\\\frac{y}{2}\\\\right)^6\\\\).\\n4. Combine everything together to get the coefficient of \\\\(x^2y^6\\\\).\\n\\nLet's compute these in Python.\\n```python\\nfrom math import comb\\n\\n# Given values\\nn = 8\\nk = 6\\n\\n# Calculate the binomial coefficient\\nbinom_coeff = comb(n, k)\\n\\n# Compute (3/5)^2\\na_term = (3/5)**2\\n\\n# Compute (-1/2)^6\\nb_term = (-1/2)**6\\n\\n# Combine terms to get the coefficient of x^2y^6\\ncoefficient = binom_coeff * a_term * b_term\\nprint(coefficient)\\n```\\n```output\\n0.1575\\n```\\nThe coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\(\\\\left(\\\\frac{3}{5}x - \\\\frac{y}{2}\\\\right)^8\\\\) is \\\\(0.1575\\\\). To express this as a common fraction, we recognize that:\\n\\n\\\\[ 0.1575 = \\\\frac{1575}{10000} = \\\\frac{63}{400} \\\\]\\n\\nThus, the coefficient can be expressed as:\\n\\n\\\\[\\n\\\\boxed{\\\\frac{63}{400}}\\n\\\\]\",\n",
       " 'prompt': [{'content': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>',\n",
       "   'role': 'system'},\n",
       "  {'content': 'What is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.',\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cold-start SFT data prep (few-shot long-CoT style)\n",
    "import re\n",
    "\n",
    "COLD_START_SYSTEM_PROMPT = (\n",
    "    \"You are a math tutor. Solve step-by-step, then give the final result. \"\n",
    "    \"Use <think>...</think> for reasoning and <answer>...</answer> for final answer.\"\n",
    ")\n",
    "\n",
    "FEW_SHOT_COT_EXAMPLES = [\n",
    "    {\n",
    "        \"question\": \"What is 2 + 3 * 4?\",\n",
    "        \"think\": \"Apply order of operations: 3*4=12, then 2+12=14.\",\n",
    "        \"answer\": \"14\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Solve 5x - 10 = 0.\",\n",
    "        \"think\": \"Add 10 on both sides to get 5x=10, then divide by 5.\",\n",
    "        \"answer\": \"x = 2\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def extract_final_answer(solution_text: str) -> str:\n",
    "    boxed = re.findall(r\"\\\\boxed\\{([^}]*)\\}\", solution_text)\n",
    "    if boxed:\n",
    "        return boxed[-1].strip()\n",
    "\n",
    "    marker = re.findall(r\"####\\s*(.+)\", solution_text)\n",
    "    if marker:\n",
    "        return marker[-1].strip()\n",
    "\n",
    "    lines = [line.strip() for line in solution_text.splitlines() if line.strip()]\n",
    "    return lines[-1] if lines else solution_text.strip()\n",
    "\n",
    "\n",
    "def make_cold_start_text(example):\n",
    "    messages = [{\"role\": \"system\", \"content\": COLD_START_SYSTEM_PROMPT}]\n",
    "\n",
    "    for ex in FEW_SHOT_COT_EXAMPLES:\n",
    "        messages.append({\"role\": \"user\", \"content\": ex[\"question\"]})\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"<think>{ex['think']}</think>\\n<answer>{ex['answer']}</answer>\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": example[\"problem\"]})\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": (\n",
    "                f\"<think>{example['solution']}</think>\\n\"\n",
    "                f\"<answer>{extract_final_answer(example['solution'])}</answer>\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "cold_start_limit = min(256, len(dataset[\"train\"]))  # Mac-safe debug size\n",
    "cold_start_sft_train = dataset[\"train\"].select(range(cold_start_limit)).map(\n",
    "    make_cold_start_text,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "print(f\"Cold-start SFT examples: {len(cold_start_sft_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview one cold-start SFT sample\n",
    "print(cold_start_sft_train[0][\"text\"][:1500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cold-start SFT trainer (version + Mac compatible)\n",
    "import inspect\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sft_device = torch.device(\"cuda\")\n",
    "    sft_dtype = torch.bfloat16\n",
    "elif torch.backends.mps.is_available():\n",
    "    sft_device = torch.device(\"mps\")\n",
    "    sft_dtype = torch.float16\n",
    "else:\n",
    "    sft_device = torch.device(\"cpu\")\n",
    "    sft_dtype = torch.float32\n",
    "\n",
    "print(f\"SFT device: {sft_device} | dtype: {sft_dtype}\")\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=sft_dtype,\n",
    ").to(sft_device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "cold_start_output_dir = \"./qwen2_cold_start_sft\"\n",
    "\n",
    "raw_sft_args = {\n",
    "    \"output_dir\": cold_start_output_dir,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"logging_steps\": 5,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 50,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"report_to\": \"none\",\n",
    "    \"bf16\": False,\n",
    "    \"fp16\": False,\n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"remove_unused_columns\": False,\n",
    "}\n",
    "\n",
    "accepted_args = set(inspect.signature(TrainingArguments.__init__).parameters)\n",
    "sft_args = TrainingArguments(**{k: v for k, v in raw_sft_args.items() if k in accepted_args})\n",
    "\n",
    "sft_init_params = set(inspect.signature(SFTTrainer.__init__).parameters)\n",
    "trainer_kwargs = {\n",
    "    \"model\": sft_model,\n",
    "    \"args\": sft_args,\n",
    "    \"train_dataset\": cold_start_sft_train,\n",
    "}\n",
    "\n",
    "if \"tokenizer\" in sft_init_params:\n",
    "    trainer_kwargs[\"tokenizer\"] = tokenizer\n",
    "elif \"processing_class\" in sft_init_params:\n",
    "    trainer_kwargs[\"processing_class\"] = tokenizer\n",
    "\n",
    "if \"dataset_text_field\" in sft_init_params:\n",
    "    trainer_kwargs[\"dataset_text_field\"] = \"text\"\n",
    "elif \"formatting_func\" in sft_init_params:\n",
    "    trainer_kwargs[\"formatting_func\"] = lambda ex: ex[\"text\"]\n",
    "\n",
    "if \"max_seq_length\" in sft_init_params:\n",
    "    trainer_kwargs[\"max_seq_length\"] = 1024\n",
    "if \"packing\" in sft_init_params:\n",
    "    trainer_kwargs[\"packing\"] = False\n",
    "\n",
    "cold_start_trainer = SFTTrainer(**trainer_kwargs)\n",
    "print(\"Cold-start SFT trainer ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save cold-start SFT checkpoint\n",
    "cold_start_trainer.train()\n",
    "cold_start_trainer.save_model(cold_start_output_dir)\n",
    "tokenizer.save_pretrained(cold_start_output_dir)\n",
    "\n",
    "GRPO_BASE_MODEL = cold_start_output_dir\n",
    "print(f\"Saved cold-start model to: {cold_start_output_dir}\")\n",
    "print(f\"GRPO_BASE_MODEL set to: {GRPO_BASE_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test from the cold-start SFT model\n",
    "sft_model.eval()\n",
    "\n",
    "test_question = \"If 3x + 5 = 20, what is x?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": COLD_START_SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": test_question},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(sft_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = sft_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=96,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if the model's response is mathematically \n",
    "    equivalent to the ground truth solution.\n",
    "    Uses latex2sympy2 for parsing and math_verify for validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract responses\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "\n",
    "    solutions = kwargs.get(\"solution\") # Get solutions from kwargs\n",
    "    \n",
    "    for content, sol in zip(contents, solutions):\n",
    "        # Parse the ground truth solution\n",
    "        gold_parsed = parse(sol, extraction_mode=\"first_match\", \n",
    "                            extraction_config=[LatexExtractionConfig()])\n",
    "        \n",
    "        if gold_parsed:  # Check if parsing was successful\n",
    "            # Parse the model's answer with relaxed normalization\n",
    "            answer_parsed = parse(\n",
    "                content,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "                            equations=True,\n",
    "                            boxed=\"all\",\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode=\"first_match\",\n",
    "            )\n",
    "\n",
    "            # Reward 1.0 if correct, 0.0 if incorrect\n",
    "            reward = float(verify(answer_parsed, gold_parsed))\n",
    "        else:\n",
    "            # If ground truth cannot be parsed, assign neutral reward (0.5)\n",
    "            reward = 0.5\n",
    "            print(\"Warning: Failed to parse gold solution:\", sol)\n",
    "\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if the completion has the correct format:\n",
    "    <think>...</think> <answer>...</answer>.\n",
    "    \"\"\"\n",
    "    # Define the regex pattern for the desired format\n",
    "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
    "\n",
    "    # Extract the content from each completion\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # Check if each completion matches the pattern\n",
    "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE)\n",
    "               for content in completion_contents]\n",
    "\n",
    "    # Reward 1.0 for correct format, 0.0 otherwise\n",
    "    return [1.0 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_steps_reward(completions, **kwargs):\n",
    "    r\"\"\"\n",
    "    Reward function to encourage clear step-by-step reasoning.\n",
    "    It looks for patterns like \"Step 1:\", numbered lists, bullet points,\n",
    "    and transition words.\n",
    "    \"\"\"\n",
    "    # Regex pattern to find indicators of reasoning steps\n",
    "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
    "\n",
    "    # Extract completion contents\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # Count the number of reasoning step indicators in each completion\n",
    "    matches = [len(re.findall(pattern, content, re.MULTILINE))\n",
    "               for content in completion_contents]\n",
    "\n",
    "    # Reward is proportional to the number of reasoning steps, maxing out at 1.0\n",
    "    # We're using a \"magic number\" 3 here - encourage at least 3 steps for full reward\n",
    "    return [min(1.0, count / 3) for count in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_scaled_reward(\n",
    "    min_value_wrong: float = -0.5,\n",
    "    max_value_wrong: float = -0.1,\n",
    "    min_value_correct: float = 0.8,\n",
    "    max_value_correct: float = 1.0,\n",
    "    max_len: int = 1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a cosine scaled reward function. This function scales the accuracy reward\n",
    "    based on completion length. Shorter correct solutions get higher rewards,\n",
    "    longer incorrect solutions get less penalty.\n",
    "    \"\"\"\n",
    "    def cosine_scaled_reward(completions, solution, accuracy_rewards, **kwargs):\n",
    "        \"\"\"\n",
    "        Cosine scaled reward function that adjusts accuracy rewards based on completion length.\n",
    "        \"\"\"\n",
    "        contents = [completion[0][\"content\"] for completion in completions]\n",
    "        rewards = []\n",
    "\n",
    "        for content, sol, acc_reward in zip(contents, solution, accuracy_rewards):\n",
    "            gen_len = len(content)  # Length of the generated answer\n",
    "            progress = gen_len / max_len # How far we are to max length\n",
    "            cosine = math.cos(progress * math.pi) # Cosine value based on progress\n",
    "\n",
    "            if acc_reward > 0.5: # Assuming accuracy_reward gives ~1.0 for correct answers\n",
    "                min_value = min_value_correct\n",
    "                max_value = max_value_correct\n",
    "            else: # Incorrect answer\n",
    "                min_value = max_value_wrong  # Note the swap!\n",
    "                max_value = min_value_wrong\n",
    "\n",
    "            # Cosine scaling formula!\n",
    "            reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine)\n",
    "            rewards.append(float(reward))\n",
    "        return rewards\n",
    "    return cosine_scaled_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repetition_penalty_reward(ngram_size: int = 3, max_penalty: float = -0.1):\n",
    "    \"\"\"\n",
    "    Returns a repetition penalty reward function. Penalizes repetitions of n-grams\n",
    "    in the generated text.\n",
    "    \"\"\"\n",
    "    if max_penalty > 0:\n",
    "        raise ValueError(f\"max_penalty {max_penalty} should not be positive\")\n",
    "\n",
    "    def zipngram(text: str, ngram_size: int):\n",
    "        \"\"\"Helper function to generate n-grams from text.\"\"\"\n",
    "        words = text.lower().split() # Lowercase and split into words\n",
    "        return zip(*[words[i:] for i in range(ngram_size)]) # Create n-grams\n",
    "\n",
    "    def repetition_penalty_reward(completions, **kwargs) -> float:\n",
    "        \"\"\"\n",
    "        Repetition penalty reward function.\n",
    "        \"\"\"\n",
    "        contents = [completion[0][\"content\"] for completion in completions]\n",
    "        rewards = []\n",
    "        for completion in contents:\n",
    "            if completion == \"\": # No penalty for empty completions\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            if len(completion.split()) < ngram_size: # No penalty for short completions\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "\n",
    "            ngrams = set() # Use a set to store unique n-grams\n",
    "            total = 0\n",
    "            for ng in zipngram(completion, ngram_size): # Generate n-grams\n",
    "                ngrams.add(ng) # Add n-gram to the set (duplicates are ignored)\n",
    "                total += 1 # Count total n-grams\n",
    "\n",
    "            # Calculate scaling factor: more repetition -> higher scaling\n",
    "            scaling = 1 - len(ngrams) / total\n",
    "            reward = scaling * max_penalty # Apply penalty based on scaling\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "    return repetition_penalty_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOScriptArguments:\n",
    "    \"\"\"\n",
    "    Script arguments for GRPO training, specifically related to reward functions.\n",
    "    \"\"\"\n",
    "\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"accuracy\", \"format\"],\n",
    "        metadata={\n",
    "            \"help\": \"List of reward functions. Possible values: 'accuracy', 'format', 'reasoning_steps', 'cosine', 'repetition_penalty'\"\n",
    "        },\n",
    "    )\n",
    "    cosine_min_value_wrong: float = field(\n",
    "        default=-0.5,\n",
    "        metadata={\"help\": \"Minimum reward for cosine scaling for wrong answers\"},\n",
    "    )\n",
    "    cosine_max_value_wrong: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum reward for cosine scaling for wrong answers\"},\n",
    "    )\n",
    "    cosine_min_value_correct: float = field(\n",
    "        default=0.8,\n",
    "        metadata={\"help\": \"Minimum reward for cosine scaling for correct answers\"},\n",
    "    )\n",
    "    cosine_max_value_correct: float = field(\n",
    "        default=1.0,\n",
    "        metadata={\"help\": \"Maximum reward for cosine scaling for correct answers\"},\n",
    "    )\n",
    "    cosine_max_len: int = field(\n",
    "        default=1000,\n",
    "        metadata={\"help\": \"Maximum length for cosine scaling\"},\n",
    "    )\n",
    "\n",
    "    repetition_n_grams: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Number of n-grams for repetition penalty reward\"},\n",
    "    )\n",
    "    repetition_max_penalty: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum (negative) penalty for for repetition penalty reward\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    }
   ],
   "source": [
    "# Define TrainingArguments from transformers\n",
    "import inspect\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)  # This manually deletes the old folder\n",
    "\n",
    "raw_training_kwargs = {\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logging_steps\": 10,\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"evaluation_strategy\": \"steps\",  # fallback for older transformers versions\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 50,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"dataloader_num_workers\": 2,\n",
    "    \"seed\": 42,\n",
    "    \"bf16\": True,\n",
    "    \"push_to_hub\": False,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"report_to\": \"none\",\n",
    "    \"remove_unused_columns\": False,\n",
    "}\n",
    "\n",
    "accepted_params = set(inspect.signature(TrainingArguments.__init__).parameters)\n",
    "training_kwargs = {k: v for k, v in raw_training_kwargs.items() if k in accepted_params}\n",
    "\n",
    "# Keep only one eval strategy key depending on installed transformers version.\n",
    "if \"eval_strategy\" in training_kwargs and \"evaluation_strategy\" in training_kwargs:\n",
    "    if \"eval_strategy\" in accepted_params:\n",
    "        training_kwargs.pop(\"evaluation_strategy\", None)\n",
    "    else:\n",
    "        training_kwargs.pop(\"eval_strategy\", None)\n",
    "\n",
    "training_args = TrainingArguments(**training_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the model.\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        default=MODEL_NAME, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    model_revision: Optional[str] = field(\n",
    "        default=\"main\", metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"}\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=\"bfloat16\", metadata={\"help\": \"Override the default `torch_dtype` and load the model under this dtype.\"}\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=True, metadata={\"help\": \"Trust remote code when loading model and tokenizer.\"}\n",
    "    )\n",
    "    attn_implementation: Optional[str] = field(\n",
    "        default=\"flash_attention_2\", metadata={\"help\": \"Attention implementation to use. 'flash_attention_2' or None\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate configuration objects\n",
    "script_args = GRPOScriptArguments()\n",
    "model_args = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Field(name=None,type=None,default=<dataclasses._MISSING_TYPE object at 0x1077ce120>,default_factory=<function <lambda> at 0x124cb0670>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'help': \"List of reward functions. Possible values: 'accuracy', 'format', 'reasoning_steps', 'cosine', 'repetition_penalty'\"}),kw_only=<dataclasses._MISSING_TYPE object at 0x1077ce120>,doc=None,_field_type=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"accuracy\", \"format\"],\n",
    "        metadata={\n",
    "            \"help\": \"List of reward functions. Possible values: 'accuracy', 'format', 'reasoning_steps', 'cosine', 'repetition_penalty'\"\n",
    "        },\n",
    "    )\n",
    "reward_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected reward function: accuracy\n",
      "Selected reward function: format\n"
     ]
    }
   ],
   "source": [
    "for func in script_args.reward_funcs:\n",
    "    print(f\"Selected reward function: {func}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_functions(script_args):\n",
    "    \"\"\"\n",
    "    Returns a list of reward functions based on the script arguments.\n",
    "    \"\"\"\n",
    "    reward_funcs_list = []\n",
    "    reward_funcs_registry = {\n",
    "        \"accuracy\": accuracy_reward,  # Assuming accuracy_reward is defined in previous steps\n",
    "        \"format\": format_reward,      # Assuming format_reward is defined in previous steps\n",
    "        \"reasoning_steps\": reasoning_steps_reward, # Assuming reasoning_steps_reward is defined\n",
    "        \"cosine\": get_cosine_scaled_reward( # Assuming get_cosine_scaled_reward is defined\n",
    "            min_value_wrong=script_args.cosine_min_value_wrong,\n",
    "            max_value_wrong=script_args.cosine_max_value_wrong,\n",
    "            min_value_correct=script_args.cosine_min_value_correct,\n",
    "            max_value_correct=script_args.cosine_max_value_correct,\n",
    "            max_len=script_args.cosine_max_len,\n",
    "        ),\n",
    "        \"repetition_penalty\": get_repetition_penalty_reward( # Assuming get_repetition_penalty_reward is defined\n",
    "            ngram_size=script_args.repetition_n_grams,\n",
    "            max_penalty=script_args.repetition_max_penalty,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    for func_name in script_args.reward_funcs:\n",
    "        if func_name not in reward_funcs_registry:\n",
    "            raise ValueError(f\"Reward function '{func_name}' not found in registry.\")\n",
    "        reward_funcs_list.append(reward_funcs_registry[func_name])\n",
    "\n",
    "    return reward_funcs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A simple callback for logging training information at specific steps.\n",
    "    \"\"\"\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            if state.log_history and len(state.log_history) > 0:\n",
    "                logger.info(f\"Step {state.global_step}: Loss = {state.log_history[-1].get('loss', None)}, Learning Rate = {state.log_history[-1].get('learning_rate', None)}\")\n",
    "            else:\n",
    "                logger.info(f\"Step {state.global_step}: No logging information available yet\")\n",
    "\n",
    "def get_callbacks(training_args, model_args, script_args):\n",
    "    \"\"\"\n",
    "    Returns a list of callbacks to be used during training.\n",
    "    For now, it includes only the LoggingCallback. You can extend this to add more callbacks.\n",
    "    \"\"\"\n",
    "    callbacks = [LoggingCallback()] # Instantiate our LoggingCallback\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reward functions and callbacks\n",
    "reward_functions = get_reward_functions(script_args)\n",
    "callbacks = get_callbacks(training_args, model_args, script_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ScratchGRPOTrainer:\n",
    "    def __init__(self, model, ref_model, tokenizer, reward_funcs, training_args, script_args):\n",
    "        self.model = model\n",
    "        self.ref_model = ref_model \n",
    "        self.ref_model.eval() # Reference model is ALWAYS frozen\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reward_funcs = reward_funcs\n",
    "        self.args = training_args\n",
    "        self.script_args = script_args\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "    def compute_log_probs(self, model, input_ids, attention_mask, prompt_len):\n",
    "        \"\"\"\n",
    "        Calculates log-probs for the 'completion' tokens ONLY.\n",
    "        \"\"\"\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # [Batch*G, SeqLen, Vocab]\n",
    "        \n",
    "        # Shift so logit at i predicts token at i+1\n",
    "        logits = logits[:, :-1, :] \n",
    "        labels = input_ids[:, 1:] \n",
    "        \n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        per_token_log_probs = torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        # KEY FIX: Mask out the prompt tokens. We only care about the answer.\n",
    "        # Everything before 'prompt_len - 1' is the question.\n",
    "        return per_token_log_probs[:, (prompt_len - 1):]\n",
    "\n",
    "    def train_step(self, batch_prompts, batch_solutions):\n",
    "        G = 8 # Group size\n",
    "        device = self.model.device\n",
    "        \n",
    "        # 1. GENERATION (Done only ONCE per step)\n",
    "        # We take the first prompt in the batch for this demo\n",
    "        prompt = batch_prompts[0]\n",
    "        solution = batch_solutions[0]\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Generate G completions\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=256,\n",
    "                num_return_sequences=G,\n",
    "                do_sample=True,\n",
    "                temperature=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            ) # Shape: [G, SeqLen]\n",
    "\n",
    "        # 2. REWARDS\n",
    "        # Decode only the NEWLY generated tokens for scoring\n",
    "        completion_ids = output_ids[:, prompt_len:]\n",
    "        completions_text = self.tokenizer.batch_decode(completion_ids, skip_special_tokens=True)\n",
    "        \n",
    "        formatted_completions = [[{\"content\": text}] for text in completions_text]\n",
    "        \n",
    "        rewards = torch.zeros(G, device=device)\n",
    "        for func in self.reward_funcs:\n",
    "            scores = func(completions=formatted_completions, solution=[solution]*G)\n",
    "            rewards += torch.tensor(scores, device=device)\n",
    "\n",
    "        # 3. ADVANTAGES (The \"Secret Sauce\" of GRPO)\n",
    "        # We normalize rewards within the group of G\n",
    "        advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8) # Shape: [G]\n",
    "\n",
    "        # 4. LOG-PROBS (Training the 'Brain')\n",
    "        # Get log-probs from the model we are training\n",
    "        curr_log_probs = self.compute_log_probs(self.model, output_ids, None, prompt_len)\n",
    "        \n",
    "        # Get log-probs from the frozen reference model\n",
    "        with torch.no_grad():\n",
    "            ref_log_probs = self.compute_log_probs(self.ref_model, output_ids, None, prompt_len)\n",
    "\n",
    "        # 5. GRPO LOSS CALCULATION\n",
    "        # Ratio of new policy vs old policy\n",
    "        ratio = torch.exp(curr_log_probs - ref_log_probs) # Shape: [G, CompletionLen]\n",
    "        \n",
    "        # Combine with advantages (Broadcasting [G] across [G, CompletionLen])\n",
    "        surrogate_loss = -(ratio * advantages.unsqueeze(1)).mean()\n",
    "\n",
    "        # KL Penalty: Don't let the model drift too far from the reference\n",
    "        kl_div = torch.exp(ref_log_probs - curr_log_probs) - (ref_log_probs - curr_log_probs) - 1\n",
    "        kl_loss = 0.1 * kl_div.mean()\n",
    "\n",
    "        total_loss = surrogate_loss + kl_loss\n",
    "\n",
    "        # 6. BACKPROPAGATION\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize models and tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "grpo_base_model = GRPO_BASE_MODEL if \"GRPO_BASE_MODEL\" in globals() else \"Qwen/Qwen2-0.5B\"\n",
    "print(f\"GRPO base model: {grpo_base_model}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    grpo_device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    grpo_device = torch.device(\"mps\")\n",
    "else:\n",
    "    grpo_device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"GRPO device: {grpo_device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(grpo_base_model, trust_remote_code=True).to(grpo_device)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(grpo_base_model, trust_remote_code=True).to(grpo_device)\n",
    "ref_model.eval()  # Reference model never trains\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Instantiate your custom trainer\n",
    "trainer = ScratchGRPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    reward_funcs=[accuracy_reward],\n",
    "    training_args=training_args,\n",
    "    script_args=script_args,\n",
    ")\n",
    "\n",
    "# 3. Build a simple dataloader for the custom trainer\n",
    "def collate_batch(examples):\n",
    "    return {\n",
    "        \"prompts\": [ex[\"problem\"] for ex in examples],\n",
    "        \"solutions\": [ex[\"solution\"] for ex in examples],\n",
    "    }\n",
    "\n",
    "train_dataset = dataset[\"train\"] if isinstance(dataset, dict) else dataset\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,  # Mac-safe debug batch size\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# 4. The training loop (short debug loop)\n",
    "epochs = 1\n",
    "max_steps_per_epoch = 20\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        if step >= max_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "        loss = trainer.train_step(\n",
    "            batch_prompts=batch[\"prompts\"],\n",
    "            batch_solutions=batch[\"solutions\"],\n",
    "        )\n",
    "        if step % 2 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model + tokenizer\n",
    "save_dir = \"./grpo_qwen2_0.5b_finetuned\"\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"Saved model to: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and test generation\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "test_dir = \"./grpo_qwen2_0.5b_finetuned\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(test_dir)\n",
    "test_model = AutoModelForCausalLM.from_pretrained(test_dir).to(device)\n",
    "test_model.eval()\n",
    "\n",
    "prompt = \"Solve: If 3x + 5 = 20, what is x?\"\n",
    "\n",
    "inputs = test_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = test_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=test_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(test_tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"test\"][0]\n",
    "print(\"Q:\", sample[\"problem\"])\n",
    "print(\"GT:\", sample[\"solution\"][:300], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}